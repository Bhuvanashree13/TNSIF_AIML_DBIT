{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa5a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import keras\n",
    "from tensorflow.keras import layers, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras is a high-level API for building and training deep learning models. It is user-friendly, modular, and extensible.\n",
    "#keras sequential model is a linear stack of layers, where you can easily add one layer at a time to build your model.\n",
    "#adam (Adaptive Moment Estimation) is an optimization algorithm that combines the benefits of two other popular optimization algorithms: AdaGrad and RMSProp. It is widely used in training deep learning models due to its efficiency and effectiveness.\n",
    "#mnist is a dataset of handwritten digits (0-9) that is commonly used for training and testing image processing systems. It contains 60,000 training images and 10,000 testing images, each of size 28x28 pixels.\n",
    "#sparse_categorical_crossentropy is a loss function used in multi-class classification problems where the target labels are integers representing the class indices. It is particularly useful when the number of classes is large, as it avoids the need to one-hot encode the target labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f4651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN stands for Recurrent Neural Network, which is a type of neural network designed to handle sequential data. RNNs have connections that form directed cycles, allowing them to maintain a hidden state that captures information from previous time steps. This makes them well-suited for tasks such as language modeling, speech recognition, and time series prediction.\n",
    "#LSTM stands for Long Short-Term Memory, which is a type of RNN architecture that\n",
    "#ReLu stands for Rectified Linear Unit, which is a popular activation function used in neural networks. It is defined as f(x) = max(0, x), meaning that it outputs the input value if it is positive, and zero otherwise. ReLU is computationally efficient and helps to mitigate the vanishing gradient problem, making it a preferred choice for deep learning models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
